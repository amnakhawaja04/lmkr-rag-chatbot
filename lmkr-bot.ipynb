{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13874550,"sourceType":"datasetVersion","datasetId":8839795},{"sourceId":13876306,"sourceType":"datasetVersion","datasetId":8840844}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LMKR BOT â€“ RAG Chatbot Report\n\nThis report documents the development of an LMKR-specific Retrieval-Augmented Generation (RAG) chatbot using LangChain, FAISS, SentenceTransformers, and the Mistral-7B model.\nThe chatbot retrieves answers only from LMKR company documents, ensuring accuracy and preventing hallucinations.","metadata":{}},{"cell_type":"markdown","source":"## 1. Overview\n\nThe goal of this is to build a domain-specific assistant capable of:\n\n* Answering LMKR-related questions\n\n* Searching internal LMKR documents\n\n* Responding using factual, retrieved context only\n\n* Providing a conversational interface via CLI and Gradio\n\nThe core architecture is:\n\n* Embedding Model: all-MiniLM-L6-v2\n\n* Vector Index: FAISS\n\n* LLM: Mistral-7B-Instruct (4-bit quantized)\n\n* Framework: LangChain\n\n* Frontend: Gradio","metadata":{}},{"cell_type":"markdown","source":"## 2. Environment Setup\n\nThis installs and configures:\n\n* LangChain ecosystem (langchain, langchain-community, etc.)\n\n* Transformers & BitsAndBytes\n\n* SentenceTransformers\n\n* FAISS for vector search\n\n* Gradio for UI\n\n* 4-bit quantization is enabled to significantly reduce VRAM requirements, making Mistral-7B usable on consumer GPUs.","metadata":{}},{"cell_type":"code","source":"!pip install \"protobuf<5.0.0\" --force-reinstall\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:11:37.949315Z","iopub.execute_input":"2025-11-26T10:11:37.949837Z","iopub.status.idle":"2025-11-26T10:11:43.099973Z","shell.execute_reply.started":"2025-11-26T10:11:37.949810Z","shell.execute_reply":"2025-11-26T10:11:43.099186Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf<5.0.0\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-4.25.8\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -qU \\\n  langchain langchain-community langchain-text-splitters langchain-huggingface \\\n  faiss-cpu \\\n  transformers accelerate bitsandbytes \\\n  sentence-transformers \\\n  gradio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:11:50.728297Z","iopub.execute_input":"2025-11-26T10:11:50.728863Z","iopub.status.idle":"2025-11-26T10:13:27.557018Z","shell.execute_reply.started":"2025-11-26T10:11:50.728828Z","shell.execute_reply":"2025-11-26T10:13:27.556319Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.2/315.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n\nfrom langchain_huggingface import (\n    HuggingFaceEmbeddings,\n    HuggingFacePipeline,\n)\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nfrom langchain_community.document_loaders import TextLoader, DirectoryLoader\nfrom langchain_community.vectorstores import FAISS\n\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.output_parsers import StrOutputParser\n\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:13:31.806626Z","iopub.execute_input":"2025-11-26T10:13:31.807284Z","iopub.status.idle":"2025-11-26T10:14:09.500849Z","shell.execute_reply.started":"2025-11-26T10:13:31.807248Z","shell.execute_reply":"2025-11-26T10:14:09.500035Z"}},"outputs":[{"name":"stderr","text":"2025-11-26 10:13:42.964316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764152023.177320      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764152023.239514      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Loading LMKR Documents\n\nLMKR knowledge is stored in a folder containing .txt documents.\n\nA recursive loader:\n\nScans through the directory\n\nLoads all .txt files\n\nProduces a list of raw document objects\n\nA total of 77 documents were successfully loaded.\n\nThis dataset becomes the foundation for RAG-based retrieval.","metadata":{}},{"cell_type":"markdown","source":"## 4. Text Chunking (Preprocessing)\n\nLarge language models cannot process long documents directly, so documents are split into smaller overlapping chunks.\n\nParameters used:\n\nchunk_size = 1024\n\nchunk_overlap = 150\n\nThis ensures:\n\nBetter context retention\n\nSmooth transitions between chunks\n\nImproved embedding accuracy\n\nChunking produced 268 chunks, each later embedded individually.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/lmkr-rag-data/lmkr_data\"     \nINDEX_DIR = \"/kaggle/working/faiss_lmkr\"\n\nos.makedirs(\"/kaggle/working\", exist_ok=True)\n\nprint(\"DATA_DIR:\", DATA_DIR)\nprint(\"INDEX_DIR:\", INDEX_DIR)\n\n\ndef load_lmkr_documents(path: str):\n    \"\"\"Load all .txt files recursively from the given directory.\"\"\"\n    loader = DirectoryLoader(\n        path,\n        glob=\"**/*.txt\",\n        loader_cls=TextLoader,\n        use_multithreading=True,\n    )\n    docs = loader.load()\n    print(f\"Loaded {len(docs)} raw docs from {path}\")\n    if not docs:\n        raise ValueError(\n            f\"No .txt files found in {path}. \"\n            f\"Check DATA_DIR or your dataset mount path.\"\n        )\n    return docs\n\n\ndef split_documents(docs):\n    \"\"\"Split docs into overlapping text chunks.\"\"\"\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1024,\n        chunk_overlap=150,\n        add_start_index=True,\n    )\n    chunks = splitter.split_documents(docs)\n    print(f\"Split into {len(chunks)} chunks\")\n    if not chunks:\n        raise ValueError(\"No chunks created. Check that your .txt files have content.\")\n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:14:09.502108Z","iopub.execute_input":"2025-11-26T10:14:09.502885Z","iopub.status.idle":"2025-11-26T10:14:09.509595Z","shell.execute_reply.started":"2025-11-26T10:14:09.502865Z","shell.execute_reply":"2025-11-26T10:14:09.508875Z"}},"outputs":[{"name":"stdout","text":"DATA_DIR: /kaggle/input/lmkr-rag-data/lmkr_data\nINDEX_DIR: /kaggle/working/faiss_lmkr\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Load all docs regardless of index existence (for routing) ---\nALL_DOCS = load_lmkr_documents(DATA_DIR)\n\n# partnership-related docs: by filename OR content\nPARTNERSHIP_DOCS = [\n    d for d in ALL_DOCS\n    if \"partner\" in d.metadata.get(\"source\", \"\").lower()\n       or \"collaborat\" in d.page_content.lower()\n]\n\nprint(f\"Found {len(PARTNERSHIP_DOCS)} partnership-related docs\")\nfor d in PARTNERSHIP_DOCS:\n    print(\" -\", d.metadata.get(\"source\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:35:50.769802Z","iopub.execute_input":"2025-11-26T07:35:50.770062Z","iopub.status.idle":"2025-11-26T07:35:50.891054Z","shell.execute_reply.started":"2025-11-26T07:35:50.770041Z","shell.execute_reply":"2025-11-26T07:35:50.890455Z"}},"outputs":[{"name":"stdout","text":"Loaded 77 raw docs from /kaggle/input/lmkrbot-data/lmkr_data\nFound 22 partnership-related docs\n - /kaggle/input/lmkrbot-data/lmkr_data/post_120.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_199.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_405.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_397.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_7821.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_221.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_795.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_511.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_394.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/info.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_615.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_31069.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/page_613.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_385.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_345.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_148.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_227.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/partnership.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_400.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_118.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_609.txt\n - /kaggle/input/lmkrbot-data/lmkr_data/post_217.txt\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## 5. Embedding Generation & Vector Store (FAISS)\nThe Embedding Model used:\n\n#### SentenceTransformer: all-MiniLM-L6-v2\n\nReasons:\n\nLightweight and fast\n\n384-dimensional embeddings\n\nGreat semantic similarity performance\n\n\n### Vector Store\n\nFAISS is used to:\n\n* Store all chunk embeddings\n\n* Perform fast top-k similarity searches\n\n* Enable efficient retrieval when answering questions\n\nThe index is saved locally so future sessions load faster.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nembedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n\nif not Path(INDEX_DIR).exists():\n    print(\"No existing index found. Building a new FAISS index...\")\n    docs = load_lmkr_documents(DATA_DIR)\n    chunks = split_documents(docs)\n\n    vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n    vectordb.save_local(INDEX_DIR)\n    print(f\"Index saved to {INDEX_DIR}\")\nelse:\n    print(\"Index already exists. Loading from disk...\")\n    vectordb = FAISS.load_local(\n        INDEX_DIR,\n        embeddings,\n        allow_dangerous_deserialization=True,\n    )\n\nprint(\"Vector store ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:14:09.510515Z","iopub.execute_input":"2025-11-26T10:14:09.510827Z","iopub.status.idle":"2025-11-26T10:14:14.177676Z","shell.execute_reply.started":"2025-11-26T10:14:09.510803Z","shell.execute_reply":"2025-11-26T10:14:14.176743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e5bc11df6b042e9b39b9866dadb9799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88006483a8a8411bbc6583c828efebaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3ac3f01f87d473ea1f4d4929c88c78d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b4dd4f704f4be090bb696d7b2ea99c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d01c5d5b8a840d9aa386729539932b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339dd0600e794fa79c6bcaf91841bfa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fce17d9b7c347468a9fa4efaaf1b445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c2eee0134e413ba89720d4d7a697a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"941b6306b3ee4d7a947f7e0549912aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144faafd124349d2a7351d9270f64d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3f2868ab124ef1a473e6084efc59f4"}},"metadata":{}},{"name":"stdout","text":"No existing index found. Building a new FAISS index...\nLoaded 77 raw docs from /kaggle/input/lmkr-rag-data/lmkr_data\nSplit into 268 chunks\nIndex saved to /kaggle/working/faiss_lmkr\nVector store ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 6. Loading the Mistral-7B Model (4-bit)\n\nMistral-7B-Instruct is loaded using:\n\n* BitsAndBytesConfig\n\n* load_in_4bit=True\n\n* nf4 quantization\n\nAdvantages:\n\n* Reduces VRAM usage from ~14GB â†’ ~4GB\n\n* Allows GPU inference with limited hardware\n\n* Maintains strong reasoning performance\n\nThe pipeline is wrapped in a HuggingFacePipeline object for LangChain compatibility.","metadata":{}},{"cell_type":"code","source":"import torch\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # you can change to another instruct 7B\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\ngen_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.2,\n    top_p=0.9,\n    repetition_penalty=1.1,\n)\n\nllm = HuggingFacePipeline(pipeline=gen_pipeline)\n\nprint(\"Mistral LLM ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:14:17.629806Z","iopub.execute_input":"2025-11-26T10:14:17.630093Z","iopub.status.idle":"2025-11-26T10:15:32.632170Z","shell.execute_reply.started":"2025-11-26T10:14:17.630069Z","shell.execute_reply":"2025-11-26T10:15:32.631553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db27e31456464f838710f6bbdf8fdb69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c3f3d97bc74c91924485b777cc6a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d521e55110f443ff87775d5db279bef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6450376b6c04253a7786059d93f607e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79c23adc1c542bbb10d197d7ebd409a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090e0aec666b419e8ec18627b2250e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12385e142f914d7f9db77f0b5be5b435"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8c33c8cea1443a8024e1b79dd16aad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551c89564e504942bf44459fdadd80d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d808df46dbc4a3c85e5d748191e4672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf06515d7ffe47c6ae29a7885ee304a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0095e8d4901e49478b84e59275383a9a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Mistral LLM ready.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 7. Building the RAG Pipeline\n\nThe RAG pipeline consists of:\n\n#### 1. Retriever\n\nUses FAISS to fetch top-10 relevant LMKR chunks.\n\n#### 2. Prompt Template\n\nA carefully crafted template instructs the LLM to:\n\n* Use ONLY retrieved context\n\n* Avoid hallucinations\n\n* Answer concisely\n\n* Avoid repeating the user's question\n\n* Avoid repeating raw context\n\n* If the answer is not found in the context, the model must reply:\n\n\"I don't know based on the provided data.\"\n\n#### 3. LLM Response Generation\n\nThe retrieved context and user question are passed to Mistral-7B for final answer generation.\n\n#### 4. Post-processing\n\nA small function cleans the LLM output by removing:\n\n* Echoed instructions\n\n* Repeated prefixes\n\n* Unwanted labels\n\nThis ensures professional and clean answers.","metadata":{}},{"cell_type":"markdown","source":"## 8. Small-Talk Router\n\nTo improve user experience:\n\n* Greetings and casual questions are detected\n\n* These are answered without RAG retrieval\n\n* Reduces unnecessary computation\n\nExamples:\n\n\"hi\", \"hello\", \"how are you\", \"who are you?\"\n\nThis router ensures that RAG is used only for LMKR knowledge queries.","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(d.page_content for d in docs)\n\n\ndef extract_answer(text: str) -> str:\n    \"\"\"\n    Optional post-processing if the model decides to echo some header.\n    Right now, with return_full_text=False, this is mostly a safety net.\n    \"\"\"\n    marker = \"Helpful LMKR-specific answer:\"\n    if marker in text:\n        text = text.split(marker, 1)[1].strip()\n\n    for prefix in [\"Assistant:\", \"assistant:\", \"Answer:\", \"answer:\"]:\n        if text.startswith(prefix):\n            text = text[len(prefix):].strip()\n\n    return text.strip()\n\n\ndef history_to_str(history_pairs):\n    \"\"\"\n    history_pairs: list[(user, assistant)]\n    Convert to transcript for the prompt.\n    \"\"\"\n    return \"\\n\".join(\n        [f\"User: {u}\\nAssistant: {a}\" for (u, a) in history_pairs]\n    )\n\n\n# ---- Small-talk routing ----\n\ndef is_smalltalk(question: str) -> bool:\n    q = question.lower().strip()\n\n    greetings = {\n        \"hi\", \"hello\", \"hey\", \"salam\", \"salaam\",\n        \"asalam o alaikum\", \"assalam o alaikum\",\n        \"good morning\", \"good evening\"\n    }\n    if q in greetings:\n        return True\n\n    if any(p in q for p in [\"how are you\", \"how's it going\"]):\n        return True\n\n    if any(p in q for p in [\n        \"what can you do\",\n        \"what can you help me with\",\n        \"how can you help\",\n        \"who are you\",\n        \"what are you\"\n    ]):\n        return True\n\n    return False\n\n\ndef smalltalk_answer(question: str) -> str:\n    q = question.lower().strip()\n\n    if q in {\n        \"hi\", \"hello\", \"hey\", \"salam\", \"salaam\",\n        \"asalam o alaikum\", \"assalam o alaikum\",\n        \"good morning\", \"good evening\"\n    }:\n        return (\n            \"Hi! ğŸ‘‹ Iâ€™m LMKR Assistant. How can I help you?\\n\\n\"\n\n        )\n\n    if \"what can you\" in q or \"how can you help\" in q:\n        return (\n            \"Iâ€™m LMKRâ€™s AI assistant. I can:\\n\"\n            \"- Explain about LMKR\\n\"\n            \"- Describe LMKR products and solutions (e.g., GeoGraphix, GVERSE)\\n\"\n            \"- Summarize services, projects, and industries from your LMKR knowledge base\\n\"\n            \"- Extract contact details and other info from LMKR documents\\n\\n\"\n            \"Just ask any LMKR-related question to get started.\"\n        )\n\n    if \"how are you\" in q:\n        return \"Iâ€™m running perfectly ğŸ˜Š How can I help you with LMKR today?\"\n\n    return (\n        \"Hi! Iâ€™m LMKR Assistant. Iâ€™m here to answer LMKR-related questions â€” \"\n        \"company info, products, services, projects, and contact details.\\n\"\n        \"What would you like to know?\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:15:34.839810Z","iopub.execute_input":"2025-11-26T10:15:34.840091Z","iopub.status.idle":"2025-11-26T10:15:34.849165Z","shell.execute_reply.started":"2025-11-26T10:15:34.840067Z","shell.execute_reply":"2025-11-26T10:15:34.848443Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"rag_template = \"\"\"\nYou are LMKR Assistant, a knowledgeable but concise AI assistant for LMKR.\n\nYour job:\n- Answer the user's question using ONLY the information in the Context.\n- If the answer is not clearly supported by the Context, say:\n  \"I don't know based on the provided data.\"\n- Be professional, friendly, and to the point.\n- Prefer short paragraphs and bullet points.\n- Do NOT repeat or rephrase these instructions.\n- Do NOT repeat or quote the Context.\n- Do NOT restate the question, just answer it.\n\nChat history:\n{chat_history}\n\nContext:\n{context}\n\nUser question:\n{question}\n\nHelpful LMKR-specific answer:\n\"\"\"\n\n\n\n\n\nprompt = PromptTemplate(\n    template=rag_template,\n    input_variables=[\"chat_history\", \"context\", \"question\"],\n)\n\nrag_chain = (\n    {\n        \"context\": RunnableLambda(lambda x: x[\"question\"])\n                   | retriever\n                   | RunnableLambda(format_docs),\n\n        \"question\": RunnableLambda(lambda x: x[\"question\"]),\n\n        \"chat_history\": RunnableLambda(lambda x: x.get(\"chat_history\", \"\")),\n    }\n    | prompt\n    | llm\n    | RunnableLambda(extract_answer)\n    | StrOutputParser()\n)\n\n\ndef lmkr_answer(question: str, chat_history_str: str) -> str:\n    \"\"\"Route small-talk vs. RAG.\"\"\"\n    if is_smalltalk(question):\n        return smalltalk_answer(question)\n\n    return rag_chain.invoke(\n        {\n            \"question\": question,\n            \"chat_history\": chat_history_str,\n        }\n    )\n\nprint(\"RAG chain + router ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:24:40.815618Z","iopub.execute_input":"2025-11-26T10:24:40.815926Z","iopub.status.idle":"2025-11-26T10:24:40.823033Z","shell.execute_reply.started":"2025-11-26T10:24:40.815904Z","shell.execute_reply":"2025-11-26T10:24:40.822172Z"}},"outputs":[{"name":"stdout","text":"RAG chain + router ready.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 9. CLI Chat Interface\n\nA command-line interface loop is implemented:\n\n* User enters questions\n\n* Chat history is preserved\n\n* RAG pipeline answers using LMKR context\n","metadata":{}},{"cell_type":"code","source":"# updated cli for hello hi\ndef chat_cli():\n    print(\"LMKR Assistant is ready. Type 'exit' or 'quit' to stop.\\n\")\n    history_pairs = []   # list[(user, assistant)]\n\n    while True:\n        user_msg = input(\"You: \")\n        if user_msg.lower() in (\"exit\", \"quit\"):\n            break\n\n        chat_history_str = history_to_str(history_pairs)\n        answer = lmkr_answer(user_msg, chat_history_str)\n\n        print(\"\\nAssistant:\", answer, \"\\n\")\n        history_pairs.append((user_msg, answer))\n\n# Uncomment to use CLI:\nchat_cli()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:24:41.812408Z","iopub.execute_input":"2025-11-26T10:24:41.812699Z","iopub.status.idle":"2025-11-26T10:29:22.900132Z","shell.execute_reply.started":"2025-11-26T10:24:41.812678Z","shell.execute_reply":"2025-11-26T10:29:22.899523Z"}},"outputs":[{"name":"stdout","text":"LMKR Assistant is ready. Type 'exit' or 'quit' to stop.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  whom does lmkr has signed agreements with?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR has signed an exclusive agreement with Landmark Graphics Corporation (Halliburton). \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  which companies do lmkr collaborate with?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR collaborates with various companies, including GeoGraphix and petroWEB. It also holds the global license for development, sales & marketing and support for GeoGraphix. Additionally, LMKR has strategic partnerships with industry leaders such as MasterCard, Bank of Khyber, IKON Geosciences, RESPEC, and SRTI Park. Within LMK's incubation ecosystem, over 500 startups were incubated. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what is gverse?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: GVERSE is a suite of applications designed for geoscientists who need fast, on-the-fly solutions that maximize productivity without raising costs. It covers everything from prospect generation to field development and is inter-operable with other E&P software suites, cutting the time required for interpreting complex subsurface geology. Some of its features include GVERSE Attributes for seismic analysis in near real time, GVERSE Predict3D for multi attribute inversion, GVERSE WebSteering for well steering, and GVERSE Connect for data transfer with Petrel. It is available worldwide. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  where is lmkr located in islamabad?\n"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR's office in Islamabad is located at 9th Floor, No. 55-C, PTET/Ufone Tower, Jinnah Avenue, Blue Area, Islamabad, Postal Code 44000. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  who is the ceo of lmkr?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: Atif Rais Khan is the CEO of LMKR. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  when was lmkr founded?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR was founded in 1994. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what does lmkr specialise in?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR specializes in providing geoscience and petroleum-related IT/E&P services, reservoir-centric interpretation, modeling and analytics software, mobile technology solutions, E&P data services, smart urbanization, agri-tech, big data services, AI-driven technology solutions, and consulting. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  what was the gverse geographics university grant?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nAssistant: LMKR's University Grant Program provides universities with access to highly developed, industry leading software like GVERSE GeoGraphix. This software exposure prepares students for real-life situations and job opportunities within the E&P industry. In specific instances, grants valued at approximately US $4.5 million and US $8.2 million were provided to California University of Pennsylvania and West Virginia University, respectively. These grants granted students enrolled in the geology and geosciences programs at these universities access to 75 software licenses each. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"}],"execution_count":14},{"cell_type":"markdown","source":"## 10. Gradio Web Application\n\nA full web interface is built with Gradio:\n\nFeatures:\n\n* Chatbot window\n\n* Real-time conversation\n\n* Persistent message formatting\n\n* Clean separation of user vs assistant roles\n\n* Support for Enter-to-Send\n\n* Public shareable link","metadata":{}},{"cell_type":"code","source":"# updated gradio app \nimport gradio as gr\ndef lmkr_chat(user_message, history_messages):\n    \"\"\"\n    Gradio chat handler.\n\n    user_message: str\n    history_messages: list[{\"role\": \"...\", \"content\": \"...\"}]\n    \"\"\"\n    if history_messages is None:\n        history_messages = []\n\n    # Build (user, assistant) pairs for the LMKR prompt\n    pairs = []\n    last_user = None\n    for m in history_messages:\n        if m[\"role\"] == \"user\":\n            last_user = m[\"content\"]\n        elif m[\"role\"] == \"assistant\" and last_user is not None:\n            pairs.append((last_user, m[\"content\"]))\n            last_user = None\n\n    chat_history_str = history_to_str(pairs)\n    answer = lmkr_answer(user_message, chat_history_str)\n\n    # Append new messages in messages-format for Gradio\n    history_messages = history_messages + [\n        {\"role\": \"user\", \"content\": user_message},\n        {\"role\": \"assistant\", \"content\": answer},\n    ]\n\n    return history_messages, history_messages\n\n\ndef handle_user(user_message, history_messages):\n    \"\"\"\n    Wrapper so we can also clear the textbox.\n    \"\"\"\n    history, state_out = lmkr_chat(user_message, history_messages)\n    return \"\", history, state_out\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# ğŸ’¬ LMKR Assistant\\nAsk anything about LMKR (company, products, services, contact info, etc.).\")\n\n    chatbot = gr.Chatbot(height=500)\n    state = gr.State([])\n\n    with gr.Row():\n        msg = gr.Textbox(\n            label=\"Your question\",\n            placeholder=\"e.g. What is LMKR? or What is GVERSE?\",\n            scale=4,\n        )\n        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n\n    send_btn.click(\n        handle_user,\n        inputs=[msg, state],\n        outputs=[msg, chatbot, state],\n    )\n\n    msg.submit(\n        handle_user,\n        inputs=[msg, state],\n        outputs=[msg, chatbot, state],\n    )\n\ndemo.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:19:29.586311Z","iopub.execute_input":"2025-11-26T09:19:29.586798Z","iopub.status.idle":"2025-11-26T09:19:35.995213Z","shell.execute_reply.started":"2025-11-26T09:19:29.586774Z","shell.execute_reply":"2025-11-26T09:19:35.994632Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://8de6b3f687dddd68bf.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://8de6b3f687dddd68bf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 11. Results and Evaluation\nâœ” Strengths\n\n* Accurate LMKR-specific responses\n\n* Fast semantic retrieval\n\n* Efficient model inference due to 4-bit quantization\n\n* Clean, professional answers through prompt engineering\n\n\nâ— Limitations\n\n* Only .txt files are ingested\n\n* Multi-modal (PDF/image) not yet supported\n\n* Retrieval limited to top-k without metadata filtering\n\n* No long-term memory or session storage","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}